{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Neuron and activation function \n",
    "\n",
    "The basic component of a neural network is the neuron, which is associated with weight $\\vec{w}$ and bias $b$. The weight $\\vec{w}$ represents the importance of the respective inputs to the output and the bias $b$ is a threshold which determines whether the neuron is activated or not.\n",
    "<br>\n",
    "The neuron takes an input vector $\\vec{x}$ and generates an output value $y$ according to some activation functions as sketched in Figure \"Schematic of a neuron\" (For simplicity, we will use $w$ and $x$ instead of their vector form in later discussion).\n",
    "\n",
    "<table class=\"image\">\n",
    "<caption align=\"center\">Schematic of a neuron</caption>\n",
    "<tr><td><img src=\"neuron_2.png\",width=400,height=400/></td></tr>\n",
    "</table>\n",
    "\n",
    "The most commonly used activation function is the sigmoid function. The sigmoid neuron responds to the input as $\\sigma(w \\cdot x + b)$, where $\\sigma$ is the sigmoid function \n",
    "\\begin{equation}\n",
    "\\sigma(t) = \\frac{1}{1+e^{-t}}\n",
    "\\end{equation}\n",
    "\n",
    "<table class=\"image\">\n",
    "<caption align=\"center\">Sigmoid function</caption>\n",
    "<tr><td><img src=\"sigmoid.png\",width=350,height=350/></td></tr>\n",
    "</table>\n",
    "\n",
    "Notice that $\\sigma$ function (Figure \"Sigmoid function\") is a smooth function of the inputs, weights and bias, which is crucial \n",
    "to the training of the neural networks. \n",
    "\n",
    "Another popular activation function is the rectifier activation function (ReLU), which is shown in Figure \"Rectifier Linear Activation Function\". \n",
    "\\begin{equation}\n",
    "f(x) = \\max(0,x),\n",
    "\\end{equation}\n",
    "where $x$ is the input to the neuron. \n",
    "\n",
    "<table class=\"image\">\n",
    "<caption align=\"center\">Rectifier Linear Activation Function compared with its smooth approximation, the Softplus function $\\ln (1+e^x)$</caption>\n",
    "<tr><td><img src=\"Rectifier_and_softplus_functions.png\",width=350,height=350/></td></tr>\n",
    "</table>\n",
    "\n",
    "ReLU is one-sided, computationally simple (only comparison, addition and multiplication), and does not have vanishing or exploding gradient problems as obersved in sigmoid function. ReLU enables faster and effective training of deep neural network on large and complex datasets.\n",
    "\n",
    "In the following part, we are going to use softmax as the activation function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Classifier and Loss function\n",
    "\n",
    "I will first introduce some notations. $x$ stands for the training input. The corresponding output is $y = y(x)$. $W$ denotes the collection of all weights in the network, and $b$ is all the biases. $N$ is the total number of training inputs, and $a$ is the vector of outputs from the network as the sum over all training inputs $x$. The output $a$ is a function of $x$, $W$ and $b$.\n",
    "$X = \\{x^{(1)}, ..., x^{(n)} \\}$ is the set of input examples in the training dataset, and $Y = \\{y^{(1)}, ..., y^{(n)}\\}$ is the corresponding set of labels for those input examples. The $a(x)$ represents the output of the neural network given input $x$.\n",
    "\n",
    "<table class=\"image\">\n",
    "<caption align=\"center\"></caption>\n",
    "<tr><td><img src=\"active_flow.png\",width=400,height=400/></td></tr>\n",
    "</table>\n",
    "\n",
    "We use $a_j^l$ to represent the output of the $j^{th}$ neuron in the $l^{th}$ layer.\n",
    "And it can be expressed as\n",
    "\n",
    "\\begin{equation}\n",
    "a_j^l = \\sigma (z_j^l) = \\sigma(\\sum_k^{n_{l-1}}(w_{jk}^l \\cdot a_k^{l-1}) + b_j^l),\n",
    "\\end{equation}\n",
    "\n",
    "where $\\sigma$ is the activation function, $w_{jk}^l$ is the weight from the $k^{th}$ neuron in the $(l-1)^{th}$ layer to the $j^{th}$ neuron in the $l^{th}$ layer, and $b_j^l$ is the bias of the $j^{th}$ neuron in the $l^{th}$ layer. \n",
    "We can also write $a^l = \\sigma(w^l \\cdot a^{l-1} + b^l)$, \n",
    "where $a^l = (a_j^l)_j$ is vector of outputs form neurons in layer $l$,\n",
    "$w^l = (w_{jk}^l)_{j,k}$ is matrix of all weights between layers $l-1$ and $l$, \n",
    "$b^l = (b_j^l)_j$ is vector of biases in layer $l$.\n",
    "\n",
    "\n",
    "<table class=\"image\">\n",
    "<caption align=\"center\">Feedforward network with sigmoid activation function</caption>\n",
    "<tr><td><img src=\"loss_notation.png\",width=400,height=400/></td></tr>\n",
    "</table>\n",
    "\n",
    "Now that we have a neural network, we need to find the approriate weights and biases so that the network can generate results that match the desired outputs $Y(X)$ for the inputs $X$. This is a classical optimization problem and a loss function can be defined to quantify the error. \n",
    "\n",
    "For classify problems, Softmax classifier and the cross-entropy loss function is commonly used. The cross-entropy cost function is a convex function, which is one of the main reasons we use this particular cost function for logistic regression. \n",
    "\n",
    "In the multinomial logistic regression, the Softmax function represents a probability distribution of K different possible outcomes. The Softmax function has the form\n",
    "\\begin{equation}\n",
    "P_j = P(Y = j) = \\frac{e^{z_j}}{\\sum_{m=1}^K e^{z_m}} , \\text{for j =  1, ..., K}.\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "The cross-entropy loss is defined as\n",
    "\\begin{equation}\n",
    "C = \\frac{1}{N}\\sum_{i=1}^{N} C_i,\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "C_i = \\sum_{m=1}^K-y_{im}\\log P_m = - \\log(\\frac{e^{z_j}}{\\sum_{m=1}^K e^{z_m}}),\n",
    "\\end{equation}\n",
    "where N is the number of total data set, j is the true label.\n",
    "\\begin{equation}\n",
    "  y_i= (0, \\dots 1, \\dots 0) =\\begin{cases}\n",
    "    1, & \\text{true label is j}.\\\\\n",
    "    0, & \\text{otherwise}.\n",
    "  \\end{cases}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Gradient descent\n",
    "\n",
    "The goal of training a neural network is to find weights($w$) and biases($b$) that minimize the cost function $C(w, b)$. One important technique used for this is gradient descent, which updates the parameters with steps proportional to the negative of the gradient of the cost function \n",
    "\n",
    "\\begin{eqnarray}\n",
    "w_{jk}^{l} \\leftarrow w_{jk}^{l} - \\eta \\cdot \\frac{\\partial C}{\\partial w_{jk}^l}, \\\\\n",
    "b_j^l \\leftarrow b_{j}^{l} - \\eta \\cdot \\frac{\\partial C}{\\partial b_{j}^l},\n",
    "\\end{eqnarray}\n",
    "\n",
    "where $\\eta >0$ is the learning rate.  $\\eta$ controls how big a step we take on each iteration of gradient descent.\n",
    "\n",
    "By repeating this process, eventually one can find a minimum of the cost function. This method is called full gradient descent, as weights and bias are updated only after all examples are processed.\n",
    "\n",
    "Note this cost function $C = \\frac{1}{N} \\sum_iC_i$ is an average over all training examples. To compute the gradient $\\nabla C$ one needs to compute the gradients $\\nabla C_i$ separately for each training input $x$, and average them, $\\nabla C = \\frac{1}{N} \\sum_i \\nabla C_i$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Back-propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Compute the $\\frac{\\partial C_i}{\\partial w_{jk}^L}, \\frac{\\partial C_i}{b_j^L}$ for the output layer\n",
    "\n",
    "<table class=\"image\">\n",
    "<caption align=\"center\">Output layer(Layer L)</caption>\n",
    "<tr><td><img src=\"layer_1.png\",width=400,height=400/></td></tr>\n",
    "</table>\n",
    "\n",
    "For classification problem, the neural network often employs a softmax layer.\n",
    "In a softmax layer, the activation $a_j^L$ of the $jth$ output neuron is\n",
    "\\begin{equation}\n",
    "    P_j = a_j^L = \\frac{e^{z_j^L}}{\\sum_{m=1}^{K} e^{z_m^L}},   \n",
    "\\end{equation}\n",
    "where the denominator sums over all the output neurons.\n",
    "We can interpret $a_j^L$ as the network's estimated probability that the correct label is $j$.\n",
    "\n",
    "The back-propagation algorithm marks an important milestone in the development of neural networks. \n",
    "It greatly simplifies the calculation of partial derivatives $\\partial C/\\partial w$ and $\\partial C/\\partial b$. \n",
    "The derivation is presented below. We start with the output layer, by chain rule\n",
    "\\begin{equation}\n",
    "\\frac{\\partial C_i}{\\partial w_{jk}^L} = \\sum_{m=1}^K \\frac{\\partial C_i}{\\partial a_{m}^L} \\cdot \\frac{\\partial a_m^L}{\\partial w_{jk}^L} = \\frac{\\partial C_i}{\\partial a_{j}^L} \\cdot \\frac{\\partial a_j^L}{\\partial w_{jk}^L}. \n",
    "\\end{equation}\n",
    "\n",
    "With cross-entropy loss function $C = \\frac{1}{N}\\sum_{i=1}^{N} C_i$. \n",
    "To simplify the task a bit, we consider a sample of size 1 consisting of only $x_i$;\n",
    "$C_i =  -\\log(a^L_j) = -\\log(\\frac{e^{z_j^L}}{\\sum_{m=1}^K e^{z_m^L}})$, \n",
    "which is enough as $\\frac{\\partial C}{\\partial w_{jk}^L} = \\frac{1}{N}\\sum_i\\frac{\\partial C_i}{\\partial w_{jk}^L}$ and $\\frac{\\partial C}{\\partial b_j^L}=\\frac{1}{N}\\sum_i\\frac{\\partial C_i}{\\partial b_j^L}$.\n",
    "\n",
    "After defining \n",
    "\\begin{eqnarray}\n",
    "z_j^L &=& \\sum_k^{n_{L-1}}w_{jk}^La_k^{L-1} + b_j^L, \\\\\n",
    "a_j^L &=& softmax (z_j^L) =  \\frac{e^{z_j^L}}{\\sum_{m=1}^K e^{z_m^L}},\n",
    "\\end{eqnarray}\n",
    "\n",
    "Invoking chain rule, we have\n",
    "\\begin{equation}\n",
    "\\frac{\\partial a_j^L}{\\partial w_{jk}^L} = \\sum_{m=1}^K \\frac{\\partial a_j^L}{\\partial z_{m}^L} \\cdot \\frac{\\partial z_m^L}{\\partial w_{jk}^L}.\n",
    "\\end{equation}\n",
    "\n",
    "Since $\\frac{\\partial a_j^L}{\\partial z_{m}^L}$ is \n",
    "\\begin{equation}\n",
    "  \\frac{\\partial a_j^L}{\\partial z_{m}^L}=\\begin{cases}\n",
    "    a_j^L(1-a_m^L), & \\text{if m = j}.\\\\\n",
    "    -a_j^L \\cdot a_m^L, & \\text{otherwise}.\n",
    "  \\end{cases}\n",
    "\\end{equation}\n",
    "In terms of Kronecker delta, it becomes\n",
    "\\begin{equation}\n",
    "  \\delta_{jm}=\\begin{cases}\n",
    "    1, & \\text{if m = j}.\\\\\n",
    "    0, & \\text{otherwise}.\n",
    "  \\end{cases}\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\frac{\\partial a_j^L}{\\partial z_{m}^L} = a_j^L(\\delta_{jm}-a_m^L).\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "and due to \n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial C_i}{\\partial a_j^L} &=& \\frac{\\partial (-\\log(a^L_j))}{\\partial a_j^L} = -\\frac{1}{a_j^L },\\\\\n",
    "\\frac{\\partial z_m^L}{\\partial w_{jk}^L} &=& \\delta_{mj}a_k^{L-1},\\\\ \\frac{\\partial z_m^L}{\\partial b_j^L} &=& \\delta_{mj}.\n",
    "\\end{eqnarray}\n",
    "\n",
    "we get\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial C_i}{\\partial w_{jk}^L} &=& \\frac{\\partial C_i}{\\partial a_{j}^L} \\cdot \\frac{\\partial a_j^L}{\\partial w_{jk}^L} \\\\\n",
    "&=& \\sum_{m=1}^K \\frac{\\partial C_i}{\\partial a_{j}^L} \\cdot \\frac{\\partial a_j^L}{\\partial z_{m}^L} \\cdot \\frac{\\partial z_m^L}{\\partial w_{jk}^L} \\\\\n",
    "&=& \\sum_{m=1}^K -\\frac{1}{a_j^L } a_j^L(\\delta_{jm}-a_m^L) \\delta_{mj} a_k^{L-1} \\\\\n",
    "&=& - \\sum_{m=1}^K (\\delta_{jm}-a_m^L) \\delta_{mj} a_k^{L-1}\\\\ \n",
    "&=& -(1-a_j^L)a_k^{L-1}\n",
    "\\end{eqnarray}\n",
    "\n",
    "Similarly, we can get\n",
    "\\begin{eqnarray}\n",
    "    \\frac{\\partial C_i}{\\partial b_{j}^L} &=& \\frac{\\partial C_i}{\\partial a_{j}^L} \\cdot \\frac{\\partial a_j^L}{\\partial b_{j}^L} \\\\ \n",
    "&=& \\sum_{m=1}^K \\frac{\\partial C_i}{\\partial a_j^L} \\cdot \\frac{\\partial a_j^L}{\\partial z_m^L} \\cdot \\frac{\\partial z_m^L}{\\partial b_j^L} \\nonumber \\\\\n",
    "    &=& \\sum_{m=1}^K -\\frac{1}{a_j^L } a_j^L(\\delta_{jm}-a_m^L) \\cdot \\delta_{jm} \\\\ \n",
    "    &=& -(1-a_j^L)\n",
    "\\end{eqnarray}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Compute the derivative for the layer L-1\n",
    "\n",
    "<table class=\"image\">\n",
    "<caption align=\"center\">Layer L-1</caption>\n",
    "<tr><td><img src=\"layer_2.png\",width=500,height=500/></td></tr>\n",
    "</table>\n",
    "\n",
    "By chain rule, \n",
    "\\begin{equation}\n",
    "\\frac{\\partial C_i}{\\partial w_{kq}^{L-1}} = \\sum_{m=1}^K \\frac{\\partial C_i}{\\partial a_{m}^L} \\cdot \\frac{\\partial a_m^L}{\\partial w_{kq}^{L-1}} = \\sum_{m=1}^K \\sum_{d=1}^{n_{L-1}} \\frac{\\partial C_i}{\\partial a_{m}^L} \\cdot \\frac{\\partial a_m^L}{\\partial a_d^{L-1}} \\cdot \\frac{\\partial a_d^{L-1}}{\\partial w_{kq}^{L-1}}. \n",
    "\\end{equation}\n",
    "\n",
    "- $\\frac{\\partial C_i}{\\partial a_{j}^L}$: already computed(in the ouput layer);\n",
    "- $\\frac{\\partial a_j^L}{\\partial a_k^{L-1}}$: link between layers L and L-1; ($a_j^L = \\sigma (z_j^L) = \\sigma (w_{jk}^La_k^{L-1} + b_{j}^L)$)\n",
    "\n",
    "Note for sigmoid activation function, we also have $\\sigma'(z_j^L) = a_j^L(1-a_j^L)$. Therefore,\n",
    "\n",
    "\\begin{equation} \n",
    "\\begin{split}\n",
    "\\frac{\\partial a_j^L}{\\partial a_k^{L-1}} & = \\sigma'(z_j^L) \\cdot \\frac{\\partial z_j^L}{\\partial a_k^{L-1}} \\\\\n",
    " & = \\sigma'(z_j^L) \\cdot w_{jk}^L \\\\\n",
    " & = a_j^L(1-a_j^L) \\cdot w_{jk}^L\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "- $\\frac{\\partial a_j^{L-1}}{\\partial w_{kq}^{L-1}}$: similarly computed as in the output layer. ($a_k^{L-1} = \\sigma (z_k^{L-1}) = \\sigma (w_{kq}^{L-1}a_q^{L-2} + b_{k}^{L-1})$)\n",
    "\n",
    "\\begin{equation} \n",
    "\\begin{split}\n",
    "\\frac{\\partial a_k^{L-1}}{\\partial w_{kq}^{L-1}} & = \\sigma'(z_k^{L-1}) \\cdot \\frac{\\partial z_k^{L-1}}{\\partial w_{kq}^{L-1}} \\\\\n",
    " & = \\sigma'(z_k^{L-1}) \\cdot a_q^{L-2} \\\\\n",
    " & = a_k^{L-1}(1-a_k^{L-1}) \\cdot a_q^{L-2}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Further inside layers\n",
    "\n",
    "<table class=\"image\">\n",
    "<caption align=\"center\">Hidden layers</caption>\n",
    "<tr><td><img src=\"layer_n.png\",width=500,height=500/></td></tr>\n",
    "</table>\n",
    "\n",
    "Continuing with the hidden layers, we have\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial C_i}{\\partial w_{qr}^{l}} = \\sum_{p,...,k,j} \\frac{\\partial a_q^l}{\\partial w_{qr}^l} \\frac{\\partial a_p^{l+1}}{\\partial a_{q}^l}   \\cdots \\frac{\\partial a_j^L}{\\partial a_k^{L-1}} \\frac{\\partial C_i}{\\partial a_{j}^{L}}, \\\\\n",
    "\\frac{\\partial C_i}{\\partial b_{q}^{l}} = \\sum_{p,...,k,j} \\frac{\\partial a_q^l}{\\partial b_{q}^l} \\frac{\\partial a_p^{l+1}}{\\partial a_{q}^l}   \\cdots \\frac{\\partial a_j^L}{\\partial a_k^{L-1}} \\frac{\\partial C_i}{\\partial a_{j}^{L}}.\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "Once we have the derivatives, we can invoke the SGD to train the neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 The backpropagation algorithm\n",
    "\n",
    "- Feedforward $x_i$  to obtain all neuron outputs:\n",
    "\\begin{equation}\n",
    "a^0 = x_i; a^l = \\sigma (W^la^{l-1} + b^l), \\text{for l = 1, ..., L}\n",
    "\\end{equation}\n",
    "\n",
    "- Backpropagate the network to compute:\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial C_i}{\\partial w_{qr}^{l}} = \\sum_{p,...,k,j} \\frac{\\partial a_q^l}{\\partial w_{qr}^l} \\frac{\\partial a_p^{l+1}}{\\partial a_{q}^l}   \\cdots \\frac{\\partial a_j^L}{\\partial a_k^{L-1}} \\frac{\\partial C_i}{\\partial a_{j}^{L}}, \\\\\n",
    "\\frac{\\partial C_i}{\\partial b_{q}^{l}} = \\sum_{p,...,k,j} \\frac{\\partial a_q^l}{\\partial b_{q}^l} \\frac{\\partial a_p^{l+1}}{\\partial a_{q}^l}   \\cdots \\frac{\\partial a_j^L}{\\partial a_k^{L-1}} \\frac{\\partial C_i}{\\partial a_{j}^{L}}.\n",
    "\\end{eqnarray}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Gradient descent update rule\n",
    "\n",
    "The cost function for N samples is\n",
    "\\begin{equation}\n",
    "C = \\frac{1}{N}\\sum_{i=1}^{N} C_i\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "## 5.1 Gradient descent for single sample (Stochastic gradient descent) \n",
    "\n",
    "The gradient descent uses single-sample update rule: \n",
    "- Intialize all the weights $w_{jk}^l$ and biases $b_j^l$;\n",
    "- Pick one training sample $x_i$,\n",
    "    - Use backpropagation to compute the partial derivative $\\frac{\\partial C_i}{\\partial w_{jk}^l}, \\frac{\\partial C_i}{\\partial b_j^l}$\n",
    "    - Update the weights and biases using:\n",
    "    \\begin{equation}\n",
    "    \\begin{split}\n",
    "    w_{jk}^l &\\leftarrow w_{jk}^l - \\eta \\cdot \\frac{\\partial C_i}{\\partial w_{jk}^l}, \\\\\n",
    "    b_j ^l &\\leftarrow b_j^l - \\eta \\cdot \\frac{\\partial C_i}{\\partial b_j^l}\n",
    "    \\end{split}\n",
    "    \\end{equation}\n",
    "    This completes one epoch in the training process.\n",
    "- Repeat the preceding step until convergence.\n",
    "\n",
    "\n",
    "\n",
    "## 5.2 Gradient descent for B samples (mini-batch gradient descent)\n",
    "\n",
    "- B is a subset of N, for every $i \\in B$, use backpropagation to compute the partial derivatives $\\frac{\\partial C_i}{\\partial w_{jk}^l}, \\frac{\\partial C_i}{\\partial b_j^l}$\n",
    "- Update the weights and biases using:\n",
    "    \\begin{equation}\n",
    "    \\begin{split}\n",
    "    w_{jk}^l &\\leftarrow w_{jk}^l - \\eta \\cdot \\frac{1}{B} \\sum_{i \\in B} \\frac{\\partial C_i}{\\partial w_{jk}^l}, \\\\\n",
    "    b_j ^l &\\leftarrow b_j^l - \\eta \\cdot\\frac{1}{B} \\sum_{i \\in B} \\frac{\\partial C_i}{\\partial b_j^l}\n",
    "    \\end{split}\n",
    "    \\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "## 5.3 Gradient descent for N samples (full gradient descet)\n",
    "\n",
    "- For every $i \\in N$, use backpropagation to compute the partial derivatives $\\frac{\\partial C_i}{\\partial w_{jk}^l}, \\frac{\\partial C_i}{\\partial b_j^l}$\n",
    "- Update the weights and biases using:\n",
    "    \\begin{equation}\n",
    "    \\begin{split}\n",
    "    w_{jk}^l &\\leftarrow w_{jk}^l - \\eta \\cdot \\frac{1}{N} \\sum_{i \\in N} \\frac{\\partial C_i}{\\partial w_{jk}^l}, \\\\\n",
    "    b_j ^l &\\leftarrow b_j^l - \\eta \\cdot\\frac{1}{N} \\sum_{i \\in N} \\frac{\\partial C_i}{\\partial b_j^l}\n",
    "    \\end{split}\n",
    "    \\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
